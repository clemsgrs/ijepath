# Run template (copy/edit for a concrete training run).
# This file is merged on top of:
# - configs/defaults.yaml
# - a chosen profile config

data:
  # Batch/memory/runtime (defaults from defaults.yaml).
  batch_size_per_gpu: 32  # Per-GPU microbatch. Global batch = batch_size_per_gpu * world_size.
  pin_mem: true
  num_workers: 16
  seed: 0
  wsi_backend: asap  # backend for wholeslidedata reader

  # Required run-specific paths.
  # Defaults in defaults.yaml are null and must be set for a real run.
  slide_manifest_csv: null
  slide_metadata_parquet: null
  anchor_catalog_manifest: null

  # Geometry and spacing are usually provided by the profile config.
  # Keep these unset here unless you intentionally override profile values.
  # Final model tensor sizes are derived as nearest multiples of meta.patch_size:
  # context_px = snap(round(context_fov_um / context_mpp), patch_size)
  # target_px = snap(round(target_fov_um / target_mpp), patch_size)
  context_mpp: null
  target_mpp: null
  context_fov_um: null
  target_fov_um: null
  targets_per_context: null

  # Tissue-aware target sampling (defaults from defaults.yaml).
  min_target_tissue_fraction: 0.25
  insufficient_target_policy: skip_anchor  # skip_anchor | skip_slide | lower_threshold
  min_target_tissue_fraction_floor: 0.10
  min_target_tissue_fraction_step: 0.05

  # Requested-vs-source spacing matching tolerance.
  spacing_tolerance: 0.05
  low_anchor_pass_warning_threshold: 1.0
  high_anchor_pass_warning_threshold: 5.0

meta:
  load_checkpoint: false
  architecture: vit_small
  patch_size: 16
  pred_depth: 3
  pred_emb_dim: 192
  read_checkpoint: null
  use_bfloat16: true

mask:
  num_enc_masks: 1
  num_pred_masks: null  # Null => auto-set to data.targets_per_context at config load time.
  min_keep: 16

optimization:
  ema: [0.996, 1.0]  # EMA momentum schedule [start, end] over schedule horizon.
  total_images_budget: null  # Required image budget (global images seen).
  final_lr: 1.0e-06
  final_weight_decay: 0.1
  ipe_scale: 1.0  # Schedule horizon multiplier: max(total_steps, int(ipe_scale * total_steps)).
  lr: 0.0002
  start_lr: 0.00002
  warmup: 0.1  # Warmup fraction of total_steps (warmup_steps=int(warmup * total_steps)).
  weight_decay: 0.04

training:
  log_every: 100000  # Log train metrics every N images seen.
  save_every: 1000000  # Save numbered checkpoint every N images seen (overridden by tuning.tune_every when tuning.enable=true).

tuning:
  enable: false
  seed: 0
  tune_every: 1000000  # Run tuner every N global images seen.
  run_baseline_at_zero: true
  execution:
    mode: async
    device: auto
    poll_every_steps: auto  # Auto = round(tune_every / (global_batch_size * 20)); set int to override.
  plugins: []
  early_stopping:
    enable: false
    selection:
      plugin: null
      dataset: null
      metric: null
    patience_evals: 5  # Stop after this many non-improving tuning runs.
    min_evals: 3  # Minimum number of tuning runs before stop condition is allowed.
    stop_training: false  # If true, terminate JEPA training once the early-stopping criterion is met.
    save_best_checkpoint: true
    best_checkpoint_name: best-robustness.pth.tar

logging:
  folder: outputs/run
  write_tag: jepa-run
  step_log_every_images: 0  # int images or float fraction in [0,1] (e.g. 10000 or 0.1); 0 disables cadence logs; "10%" is invalid.

wandb:
  enable: false
  project: ijepath
  username: null
  exp_name: null
  tags: []
  dir: null
  group: null
  resume_id: null
