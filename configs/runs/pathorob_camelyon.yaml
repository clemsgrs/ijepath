data:
  # slide_manifest_csv: data/tcga-prad/manifests/tcga_prad_slides_with_tissue_masks.csv
  # slide_metadata_parquet: data/tcga-prad/indexes/slide_metadata.parquet
  # anchor_catalog_manifest: data/tcga-prad/indexes/anchor_catalog_manifest.json
  slide_manifest_csv: /data/pathology/projects/clement/discern/multi-center-prostate.csv
  slide_metadata_parquet: data/3-center-prostate/indexes/slide_metadata.parquet
  anchor_catalog_manifest: data/3-center-prostate/indexes/anchor_catalog_manifest.json
  batch_size_per_gpu: 32
  num_workers: 16

meta:
  architecture: vit_small
  patch_size: 16
  pred_depth: 3
  pred_emb_dim: 192
  use_bfloat16: true
  load_checkpoint: false

optimization:
  total_images_budget: 1000000  # Total global images seen for this run.
  warmup: 0.1  # Warmup fraction of total_steps (warmup_steps=int(warmup * total_steps)).
  lr: 0.0002
  start_lr: 0.00002
  final_lr: 0.000001
  weight_decay: 0.04
  final_weight_decay: 0.1
  ema: [0.996, 1.0]  # EMA momentum schedule [start, end] over schedule horizon.

tuning:
  enable: true
  seed: 0
  tune_every: 10000  # Run tuner every N global images seen.
  run_baseline_at_zero: true
  execution:
    mode: async  # Non-blocking tuning sidecar; training no longer waits on evaluator execution.
    device: auto  # Auto-reserve one GPU for tuning (use cuda:<id> to pin manually).
    max_pending_jobs: 2  # Bound pending eval jobs so queue growth stays controlled.
    coalesce_policy: newest  # Keep newest queued eval requests when backlog occurs.
    poll_every_steps: 10  # Poll completed async tune jobs every N train steps.
    fail_on_backlog: false  # True => fail fast on full queue; false => drop stale queued evals.
    keep_last_n_snapshots: 2  # Limit on-disk teacher snapshots retained by async tuner.
  data_dir: /data/pathology/projects/clement/discern/data/eval
  plugins:
    - type: pathorob
      enable: true
      batch_size_per_gpu: ${data.batch_size_per_gpu}
      num_workers: ${data.num_workers}
      feature_num_workers: ${data.num_workers}  # Feature extraction DataLoader workers.
      feature_persistent_workers: true  # Keep feature workers alive across eval events.
      feature_prefetch_factor: 4  # Batches prefetched per feature worker.
      transforms:
        resize: 256
        crop_size: null
        normalize: imagenet
      datasets:
        camelyon:
          enable: true
          manifest_csv: ${tuning.data_dir}/pathorob/camelyon/benchmark.csv
          id_centers: ["RUMC", "UMCU"]
          ood_centers: ["CWZ", "RST", "LPON"]
        camelyon_tiny:
          enable: false
          manifest_csv: ${tuning.data_dir}/pathorob/camelyon_tiny/benchmark.csv
          id_centers: ["RUMC", "UMCU"]
          ood_centers: ["CWZ", "RST", "LPON"]
      ri:
        enable: true
        every_n_evals: 1  # Run RI at every tuning event (required if used for early stopping).
        k_candidates: [3, 5, 7, 10, 15]
      apd:
        enable: true
        every_n_evals: 5  # Heavy metric; run every 5 eval events by default.
        mode: custom
        correlation_levels: [0.0, 0.14, 0.29, 0.43, 0.57, 0.71, 0.86, 1.0]
        repetitions: 3
        id_test_fraction: 0.2
      clustering:
        enable: true
        every_n_evals: 5  # Heavy metric; run every 5 eval events by default.
        repeats: 5
        k_min: 2
        k_max: 10
  early_stopping:
    enable: false
    selection:
      plugin: pathorob
      dataset: camelyon
      metric: apd_avg
    patience_evals: 5  # Stop after this many non-improving tuning evaluations.
    min_evals: 3  # Minimum number of evaluations before stop condition is allowed.
    stop_training: false  # If true, terminate JEPA training once the early-stopping criterion is met.
    save_best_checkpoint: true
    best_checkpoint_name: best-robustness.pth.tar

training:
  log_every: 10000
  save_every: 10000

logging:
  folder: outputs/${wandb.exp_name}
  write_tag: jepa-pathorob
  step_log_every_iters: 0

wandb:
  enable: true
  exp_name: pathorob-camelyon
